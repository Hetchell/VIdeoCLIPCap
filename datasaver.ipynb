{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import skimage.io as io\n",
    "import clip\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "clip_model_type = \"ViT-B_32\"\n",
    "out_path = \"./data/Video_CLIP_ViT-B_32.pkl\"\n",
    "csv_path = \"./Charades/Charades_v1_train.csv\"\n",
    "video_path = \"./video\" \n",
    "frames_no = 10 \n",
    "num_video_batch = 5\n",
    "model, preprocess = clip.load(clip_model_type, device=device, jit=False)\n",
    "\n",
    "#extract frames\n",
    "def extract_frames(video_file):\n",
    "    video, audio, info = torchvision.io.read_video(video_file)\n",
    "\n",
    "    num_frames = info[\"video_fps\"] * info[\"video_duration\"]\n",
    "    frame_rate = info[\"video_fps\"]\n",
    "    interval = int(frame_rate * 60 / frames_no)\n",
    "    frames = []\n",
    "    to_pil = torchvision.transforms.ToPILImage()\n",
    "    for i in range(0, num_frames, interval):\n",
    "      frames.append(to_pil(video[i]))\n",
    "\n",
    "    return frames\n",
    "\n",
    "#encode using clip image encoder and stack the tensors alon a new dimension\n",
    "def preprocess_frames(frames):\n",
    "\n",
    "    tensors = []\n",
    "\n",
    "    for i in range(0,len(frames)): #should this be len()+1???\n",
    "        frame = preprocess(frames[i]).unsqueeze(0).to(device)\n",
    "        tensors.append(frame)\n",
    "\n",
    "    preprocess_batch = torch.stack(tensors)\n",
    "    preprocess_batch = preprocess_batch.to(device)\n",
    "\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    #     image_features = model.encode_image(batch)\n",
    "\n",
    "    return preprocess_batch\n",
    "    \n",
    "# Define a function to concatenate tensors \n",
    "def CLIPtokenise(tensors):\n",
    "    #the input tensors are 3d stacks (lets say x,y,z)\n",
    "    #function returns a tensor of shape (x, y, sum(z)) and a list of z values\n",
    "\n",
    "    # get the z values for each tensor\n",
    "    z_values = [t.shape[2] for t in tensors]\n",
    "    joined = torch.cat(tensors, dim=2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "            image_features = model.encode_image(joined)\n",
    "\n",
    "    #gets z values and splits them back to video stacks\n",
    "    split = torch.split(image_features, z_values, dim=2)\n",
    "\n",
    "    return split\n",
    "\n",
    "# load captions and video names\n",
    "df = pd.read_csv(csv_path)\n",
    "ids = df[\"id\"]\n",
    "\n",
    "# loop over the ids with a batch size step\n",
    "for i in tqdm(range(0, len(ids), num_video_batch)):\n",
    "    # get the current batch of ids\n",
    "    batch_ids = ids[i:i+num_video_batch]\n",
    "\n",
    "    batch = []\n",
    "\n",
    "    # loop over the batch ids\n",
    "    for id in batch_ids:\n",
    "        # get the video file name with the corresponding id\n",
    "        video_file = f\"{video_path}/{id}.mp4\"\n",
    "        batch.append(preprocess_frames(extract_frames(video_file)))\n",
    "        \n",
    "    tokenised_batch = CLIPtokenise(batch)\n",
    "\n",
    "    for j in range(len(batch_ids)):\n",
    "        torch.save(tokenised_batch[j], f\"./video_tensors/{batch_ids[j]}.pt\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clipcap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
