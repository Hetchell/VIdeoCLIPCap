{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from tqdm import tqdm, trange\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data loaders Pytorch dataset class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "d_model = 512  # Dimension of the model\n",
    "output_dim = 32  # Output dimension of the MLP\n",
    "num_layers = 6  # Number of transformer layers\n",
    "num_heads = 8  # Number of heads in multi-headed attention\n",
    "dim_feedforward = 2048  # Dimension of the feedforward network\n",
    "dropout = 0.1  # Dropout rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GPT-2 model and tokenizer\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
    "gpt2_model.eval()  # Freeze the GPT-2 model\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Freeze all the parameters\n",
    "for param in gpt2_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        L, N = x.size(10), x.size(0)\n",
    "        pos = torch.arange(L).unsqueeze(0).repest(N, 1).to(x.device)\n",
    "        pos_embedding = self.calc_pos_embedding(pos)\n",
    "        return x + pos_embedding\n",
    "    def calc_pos_embedding(self, pos):\n",
    "        pos = pos.float()\n",
    "        factor = torch.exp(-torch.arange(0, self.d_model, 2).float() * (torch.log(torch.tensor(10000.0)) / self.d_model))\n",
    "        sinusoid_inp = torch.ger(pos, factor)\n",
    "        pos_embedding = torch.cat([sinusoid_inp.sin(), sinusoid_inp.cos()], dim=-1)\n",
    "        return pos_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dim_feedforward, dropout, num_layers):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model, num_heads, dim_feedforward, dropout)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers)\n",
    "        self.output_layer = nn.Linear(d_model, output_dim)\n",
    "    \n",
    "    def forward(self, x, tgt):\n",
    "        tgt = tgt.permute(1, 0, 2)\n",
    "        output = self.transformer_decoder(tgt, x)\n",
    "        output = self.output_layer(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc1(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dim_feedforward, dropout, num_layers, output_dim):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        self.decoder = TransformerDecoder(d_model, num_heads, dim_feedforward, dropout, num_layers)\n",
    "        self.mlp = MLP(d_model, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.decoder(x)\n",
    "        x = self.mlp(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "loss_fn = F.cross_entropy()\n",
    "\n",
    "# Your training loop here\n",
    "# where you calculate the loss as per your requirement and perform backpropagation\n",
    "\n",
    "\n",
    "def Train(dataset: \"Add dataset\", model: MyModel, args,\n",
    "               lr: float = 2e-5, warmup_steps: int = 5000, output_dir: str = \".\", output_prefix: str = \"\"):\n",
    "\n",
    "    num_epochs = 10\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for i, (input_data, target) in enumerate(train_loader):\n",
    "            input_data, target = input_data.to(device), target.to(device)\n",
    "\n",
    "            # Forward pass through your model\n",
    "            output = model(input_data)\n",
    "\n",
    "            # Tokenize the output of your model\n",
    "            tokens = gpt2_tokenizer(output.tolist(), return_tensors='pt').to(device)\n",
    "\n",
    "            # Forward pass through GPT-2 model\n",
    "            gpt2_output = gpt2_model(tokens).last_hidden_state\n",
    "\n",
    "            # Remove the first 32 tokens from the GPT-2 output\n",
    "            gpt2_output = gpt2_output[:, 32:]\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = loss_fn(gpt2_output, target)\n",
    "\n",
    "            # Backpropagate loss and update parameters\n",
    "            optimizer = AdamW(model.parameters(), lr=lr)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f'Epoch: {epoch}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset: ClipCocoDataset, model: ClipCaptionModel, args,\n",
    "          lr: float = 2e-5, warmup_steps: int = 5000, output_dir: str = \".\", output_prefix: str = \"\"):\n",
    "\n",
    "    device = torch.device('cuda:0')\n",
    "    batch_size = args.bs\n",
    "    epochs = args.epochs\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=epochs * len(train_dataloader)\n",
    "    )\n",
    "    # save_config(args)\n",
    "    for epoch in range(epochs):\n",
    "        print(f\">>> Training epoch {epoch}\")\n",
    "        sys.stdout.flush()\n",
    "        progress = tqdm(total=len(train_dataloader), desc=output_prefix)\n",
    "        for idx, (tokens, mask, prefix) in enumerate(train_dataloader):\n",
    "            model.zero_grad()\n",
    "            tokens, mask, prefix = tokens.to(device), mask.to(device), prefix.to(device, dtype=torch.float32)\n",
    "            outputs = model(tokens, prefix, mask)\n",
    "            logits = outputs.logits[:, dataset.prefix_length - 1: -1]\n",
    "            loss = nnf.cross_entropy(logits.reshape(-1, logits.shape[-1]), tokens.flatten(), ignore_index=0)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress.set_postfix({\"loss\": loss.item()})\n",
    "            progress.update()\n",
    "            if (idx + 1) % 10000 == 0:\n",
    "                torch.save(\n",
    "                    model.state_dict(),\n",
    "                    os.path.join(output_dir, f\"{output_prefix}_latest.pt\"),\n",
    "                )\n",
    "        progress.close()\n",
    "        if epoch % args.save_every == 0 or epoch == epochs - 1:\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                os.path.join(output_dir, f\"{output_prefix}-{epoch:03d}.pt\"),\n",
    "            )\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clipcap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
