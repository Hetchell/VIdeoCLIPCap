{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn.functional as nnf\n",
    "import sys\n",
    "from typing import Tuple, List, Union, Optional\n",
    "\n",
    "import clip\n",
    "import os\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import skimage.io as io\n",
    "import PIL.Image\n",
    "from IPython.display import Image \n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data loaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "d_model = 512  # Dimension of the model\n",
    "output_dim = 32  # Output dimension of the MLP\n",
    "num_layers = 6  # Number of transformer layers\n",
    "num_heads = 8  # Number of heads in multi-headed attention\n",
    "dim_feedforward = 2048  # Dimension of the feedforward network\n",
    "dropout = 0.1  # Dropout rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GPT-2 model and tokenizer\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
    "gpt2_model.eval()  # Freeze the GPT-2 model\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Freeze all the parameters\n",
    "for param in gpt2_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        L, N = x.size(10), x.size(0)\n",
    "        pos = torch.arange(L).unsqueeze(0).repest(N, 1).to(x.device)\n",
    "        pos_embedding = self.calc_pos_embedding(pos)\n",
    "        return x + pos_embedding\n",
    "    def calc_pos_embedding(self, pos):\n",
    "        pos = pos.float()\n",
    "        factor = torch.exp(-torch.arange(0, self.d_model, 2).float() * (torch.log(torch.tensor(10000.0)) / self.d_model))\n",
    "        sinusoid_inp = torch.ger(pos, factor)\n",
    "        pos_embedding = torch.cat([sinusoid_inp.sin(), sinusoid_inp.cos()], dim=-1)\n",
    "        return pos_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dim_feedforward, dropout, num_layers):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model, num_heads, dim_feedforward, dropout)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers)\n",
    "        self.output_layer = nn.Linear(d_model, output_dim)\n",
    "    \n",
    "    def forward(self, x, tgt):\n",
    "        tgt = tgt.permute(1, 0, 2)\n",
    "        output = self.transformer_decoder(tgt, x)\n",
    "        output = self.output_layer(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc1(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dim_feedforward, dropout, num_layers, output_dim):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        self.decoder = TransformerDecoder(d_model, num_heads, dim_feedforward, dropout, num_layers)\n",
    "        self.mlp = MLP(d_model, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.decoder(x)\n",
    "        x = self.mlp(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "loss_fn = F.cross_entropy()\n",
    "\n",
    "# Your training loop here\n",
    "# where you calculate the loss as per your requirement and perform backpropagation\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for i, (input_data, target) in enumerate(train_loader):\n",
    "        input_data, target = input_data.to(device), target.to(device)\n",
    "\n",
    "        # Forward pass through your model\n",
    "        output = model(input_data)\n",
    "\n",
    "        # Tokenize the output of your model\n",
    "        tokens = gpt2_tokenizer(output.tolist(), return_tensors='pt').to(device)\n",
    "\n",
    "        # Forward pass through GPT-2 model\n",
    "        gpt2_output = gpt2_model(tokens).last_hidden_state\n",
    "\n",
    "        # Remove the first 32 tokens from the GPT-2 output\n",
    "        gpt2_output = gpt2_output[:, 32:]\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_fn(gpt2_output, target)\n",
    "\n",
    "        # Backpropagate loss and update parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch: {epoch}, Loss: {loss.item()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clipcap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
