{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import skimage.io as io\n",
    "import clip\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model_type = \"ViT-B/32\"\n",
    "# out_path = \"./data/Video_CLIP_ViT-B_32.pkl\"\n",
    "csv_path = \"./Charades/Charades_v1_train.csv\"\n",
    "video_path = \"./video\" \n",
    "frames_no = 10 \n",
    "num_video_batch = 5\n",
    "model, preprocess = clip.load(clip_model_type, device=device, jit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract frames\n",
    "def extract_frames(video_file):\n",
    "    video, audio, info = torchvision.io.read_video(video_file, output_format='TCHW')\n",
    "\n",
    "    num_frames = len(video)\n",
    "    frame_rate = info[\"video_fps\"]\n",
    "    interval = int(frame_rate * 60 / frames_no)\n",
    "    frames = []\n",
    "    to_pil = torchvision.transforms.ToPILImage()\n",
    "    for i in range(0, num_frames, interval):\n",
    "      frames.append(to_pil(video[i].squeeze(0)))\n",
    "      # print(np.shape(to_pil(video[i].squeeze(0))))\n",
    "    \n",
    "\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode using clip image encoder and stack the tensors alon a new dimension\n",
    "def preprocess_frames(frames):\n",
    "\n",
    "    tensors = []\n",
    "\n",
    "    for i in range(0,len(frames)): #should this be len()+1???\n",
    "        frame = preprocess(frames[i]).unsqueeze(0).to(device)\n",
    "        tensors.append(frame)\n",
    "        # print(frame.shape)\n",
    "\n",
    "    preprocess_batch = torch.cat(tensors, dim = 0)\n",
    "    preprocess_batch = preprocess_batch.to(device)\n",
    "    print(preprocess_batch.shape)\n",
    "\n",
    "    return preprocess_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to concatenate tensors \n",
    "def CLIPtokenise(tensors):\n",
    "    #the input tensors are 3d stacks (lets say x,y,z)\n",
    "    #function returns a tensor of shape (x, y, sum(z)) and a list of z values\n",
    "\n",
    "    for t in tensors:\n",
    "        print(t.shape) \n",
    "\n",
    "    # get the z values for each tensor\n",
    "    z_values = [t.shape[0] for t in tensors]\n",
    "\n",
    "    joined = torch.cat(tensors, dim=0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "            image_features = model.encode_image(joined)\n",
    "\n",
    "    image_features.to(device)\n",
    "\n",
    "    #gets z values and splits them back to video stacks\n",
    "    split = torch.split(image_features, z_values, dim=0)\n",
    "\n",
    "    return split\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dhavi\\anaconda3\\envs\\clipcap\\Lib\\site-packages\\torchvision\\io\\video.py:161: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 224, 224])\n",
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([6, 3, 224, 224])\n",
      "torch.Size([6, 3, 224, 224])\n",
      "torch.Size([6, 3, 224, 224])\n",
      "torch.Size([5, 3, 224, 224])\n",
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([6, 3, 224, 224])\n",
      "torch.Size([6, 3, 224, 224])\n",
      "torch.Size([6, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:12<00:24, 12.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 3, 224, 224])\n",
      "torch.Size([5, 3, 224, 224])\n",
      "torch.Size([6, 3, 224, 224])\n",
      "torch.Size([2, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:17<00:35, 17.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 224, 224])\n",
      "torch.Size([7, 3, 224, 224])\n",
      "torch.Size([5, 3, 224, 224])\n",
      "torch.Size([6, 3, 224, 224])\n",
      "torch.Size([2, 3, 224, 224])\n",
      "torch.Size([6, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\dhavi\\anaconda3\\envs\\clipcap\\Lib\\site-packages\\pandas\\core\\indexes\\range.py:391\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 391\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_range\u001b[39m.\u001b[39mindex(new_key)\n\u001b[0;32m    392\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "\u001b[1;31mValueError\u001b[0m: 0 is not in range",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m tokenised_batch \u001b[39m=\u001b[39m CLIPtokenise(batch)\n\u001b[0;32m     22\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(batch_ids)):\n\u001b[1;32m---> 23\u001b[0m     torch\u001b[39m.\u001b[39msave(tokenised_batch[j], \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./video_tensors/\u001b[39m\u001b[39m{\u001b[39;00mbatch_ids[j]\u001b[39m}\u001b[39;00m\u001b[39m.pt\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\dhavi\\anaconda3\\envs\\clipcap\\Lib\\site-packages\\pandas\\core\\series.py:981\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    978\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n\u001b[0;32m    980\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m--> 981\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_value(key)\n\u001b[0;32m    983\u001b[0m \u001b[39mif\u001b[39;00m is_hashable(key):\n\u001b[0;32m    984\u001b[0m     \u001b[39m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[0;32m    985\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    986\u001b[0m         \u001b[39m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dhavi\\anaconda3\\envs\\clipcap\\Lib\\site-packages\\pandas\\core\\series.py:1089\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1086\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[label]\n\u001b[0;32m   1088\u001b[0m \u001b[39m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1089\u001b[0m loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mget_loc(label)\n\u001b[0;32m   1090\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39m_get_values_for_loc(\u001b[39mself\u001b[39m, loc, label)\n",
      "File \u001b[1;32mc:\\Users\\dhavi\\anaconda3\\envs\\clipcap\\Lib\\site-packages\\pandas\\core\\indexes\\range.py:393\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m    391\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_range\u001b[39m.\u001b[39mindex(new_key)\n\u001b[0;32m    392\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m--> 393\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m    394\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n\u001b[0;32m    395\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "# load captions and video names\n",
    "# df = pd.read_csv(csv_path)\n",
    "df_new = pd.read_csv(csv_path)\n",
    "df = df_new.head(15)\n",
    "ids = df[\"id\"]\n",
    "\n",
    "# loop over the ids with a batch size step\n",
    "for i in tqdm(range(0, len(ids), num_video_batch)):\n",
    "    # get the current batch of ids\n",
    "    batch_ids = ids[i:i+num_video_batch]\n",
    "\n",
    "    batch = []\n",
    "\n",
    "    # loop over the batch ids\n",
    "    for id in batch_ids:\n",
    "        # get the video file name with the corresponding id\n",
    "        video_file = f\"{video_path}/{id}.mp4\"\n",
    "        batch.append(preprocess_frames(extract_frames(video_file)))\n",
    "        \n",
    "    tokenised_batch = CLIPtokenise(batch)\n",
    "\n",
    "    for j in range(len(batch_ids)):\n",
    "        torch.save(tokenised_batch[j], f\"./video_tensors/{batch_ids[j]}.pt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clipcap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
