{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dhavi\\anaconda3\\envs\\clipcap\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from tqdm import tqdm, trange\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #csv_file\n",
    "# #tensor_dir\n",
    "\n",
    "# #Data loaders Pytorch dataset class\n",
    "# class VideoDescriptionDataset(Dataset):\n",
    "#     def __init__(self, csv_file, tensor_dir):\n",
    "#         self.df = pd.read_csv(csv_file)\n",
    "#         self.tensor_dir = tensor_dir\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.df)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         if torch.is_tensor(idx):\n",
    "#             idx = idx.tolist()\n",
    "        \n",
    "#         tensor_file = os.path.join(self.tensor_dir, str(self.df.loc[idx, \"id\"]) + '.pt')\n",
    "#         tensor = torch.load(tensor_file)\n",
    "#         description = self.df.loc[idx, \"description\"]\n",
    "\n",
    "#         return tensor, description\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "\n",
    "# Data loaders Pytorch dataset class\n",
    "class VideoDescriptionDataset(Dataset):\n",
    "    def __init__(self, csv_file, tensor_dir, gpt2_tokenizer):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.tensor_dir = tensor_dir\n",
    "        self.gpt2_tokenizer = gpt2_tokenizer\n",
    "        self.error = []\n",
    "\n",
    "        print(\"test\", len(self.df), self.tensor_dir, self.gpt2_tokenizer)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            # print(\"A\")\n",
    "\n",
    "        print(\"B\")\n",
    "\n",
    "        tensor_file = (self.tensor_dir + str(self.df.loc[idx, \"id\"]) + '.pt')\n",
    "        print(\"id is\" + str(self.df.loc[idx, \"id\"]))\n",
    "        # print(f\"tensor file is :{tensor_file}\")\n",
    "\n",
    "        print(\"C\")\n",
    "\n",
    "        try:\n",
    "            print(\"D\")\n",
    "            tensor = torch.load(tensor_file)\n",
    "            print(\"E\")\n",
    "            description = self.df.loc[idx, \"descriptions\"]\n",
    "            print(\"F\")\n",
    "            # Tokenize the description using GPT-2 tokenizer\n",
    "            tokens = self.gpt2_tokenizer(description, return_tensors='pt').to(device)\n",
    "            print(\"G\")\n",
    "            \n",
    "            return tensor, tokens[\"input_ids\"]\n",
    "        except Exception as e:\n",
    "            print(\"H\")\n",
    "            print(f\"Error loading data for index {idx}: {str(e)}\")\n",
    "            self.error.append(str(self.df.loc[idx, \"id\"]))\n",
    "            return torch.empty(0),torch.empty(0)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "d_model = 512  # Dimension of the model\n",
    "output_dim = 32  # Output dimension of the MLP\n",
    "num_layers = 6  # Number of transformer layers\n",
    "num_heads = 8  # Number of heads in multi-headed attention\n",
    "dim_feedforward = 2048  # Dimension of the feedforward network\n",
    "dropout = 0.1  # Dropout rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GPT-2 model and tokenizer\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
    "gpt2_model.eval()  # Freeze the GPT-2 model\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PositionalEncoding(nn.Module):\n",
    "#     def __init__(self, d_model):\n",
    "#         super(PositionalEncoding, self).__init__()\n",
    "#         self.d_model = d_model\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         L, N = x.size(1), x.size(0)\n",
    "#         pos = torch.arange(L).unsqueeze(0).repeat(N, 1).to(x.device)\n",
    "#         pos_embedding = self.calc_pos_embedding(pos)\n",
    "#         return x + pos_embedding\n",
    "    \n",
    "#     def calc_pos_embedding(self, pos):\n",
    "#         pos = pos.float()\n",
    "#         factor = torch.exp(-torch.arange(0, self.d_model, 2).float() * (torch.log(torch.tensor(10000.0)) / self.d_model))\n",
    "#         sinusoid_inp = torch.ger(pos, factor)\n",
    "#         pos_embedding = torch.cat([sinusoid_inp.sin(), sinusoid_inp.cos()], dim=-1)\n",
    "#         return pos_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model).to(device)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).to(device)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(torch.log(torch.tensor(10000.0)) / d_model)).to(device)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dim_feedforward, dropout, num_layers):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model, num_heads, dim_feedforward, dropout)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers)\n",
    "        self.output_layer = nn.Linear(d_model, output_dim)\n",
    "    \n",
    "    def forward(self, x, tgt):\n",
    "        tgt = tgt.permute(1, 0, 2)\n",
    "        output = self.transformer_decoder(tgt, x)\n",
    "        output = self.output_layer(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc1(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dim_feedforward, dropout, num_layers, output_dim):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        self.decoder = TransformerDecoder(d_model, num_heads, dim_feedforward, dropout, num_layers)\n",
    "        self.mlp = MLP(d_model, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.decoder(x)\n",
    "        x = self.mlp(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 7983 C:/Users/dhavi/Downloads/VIdeoCLIPCap/video_tensors_120/ GPT2Tokenizer(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True)}, clean_up_tokenization_spaces=True)\n"
     ]
    }
   ],
   "source": [
    "# Loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "csv_file = \"C:/Users/dhavi/Downloads/VIdeoCLIPCap/Charades/Charades_gpt_train.csv\"\n",
    "pt_path = \"C:/Users/dhavi/Downloads/VIdeoCLIPCap/video_tensors_120/\"\n",
    "\n",
    "dataset = VideoDescriptionDataset(csv_file, pt_path, gpt2_tokenizer)\n",
    "\n",
    "# Training loop\n",
    "def Train(dataset: dataset, model: MyModel, lr: float = 2e-5, warmup_steps: int = 5000, output_dir: str = \".\", output_prefix: str = \"\"):\n",
    "    \n",
    "    # Create a SummaryWriter\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    print(\"1\")\n",
    "\n",
    "\n",
    "    # Initialize the DataLoader\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=True)#, num_workers=2)\n",
    "\n",
    "    print(\"2\")\n",
    "\n",
    "\n",
    "    num_epochs = 10\n",
    "    # Optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    print(\"3\")\n",
    "\n",
    "\n",
    "    # Scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=warmup_steps, num_training_steps=num_epochs * len(dataloader)\n",
    "        )\n",
    "    \n",
    "    print(3.5)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        print(3.75)\n",
    "        # Use tqdm to show progress bar\n",
    "        for i, (input_data, target) in tqdm(enumerate(dataloader)):\n",
    "\n",
    "            if (input_data.shape == torch.Size([])) or (target.shape == torch.Size([])):\n",
    "                print(f\"Skipping batch {i} due to error loading data\")\n",
    "                continue\n",
    "\n",
    "\n",
    "            print(\"3.8\")\n",
    "            input_data, target = input_data.to(device), target.to(device)\n",
    "\n",
    "            print(\"4\")\n",
    "\n",
    "            # Forward pass through your model\n",
    "            output = model(input_data)\n",
    "\n",
    "            print(\"5\")\n",
    "\n",
    "\n",
    "            # Forward pass through GPT-2 model\n",
    "            gpt2_output = gpt2_model(output).last_hidden_state\n",
    "\n",
    "            print(\"6\")  \n",
    "\n",
    "\n",
    "            # Remove the first 32 tokens from the GPT-2 output\n",
    "            gpt2_output = gpt2_output[:, 32:]\n",
    "\n",
    "            print(\"7\")\n",
    "\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = loss_fn(gpt2_output, target)\n",
    "            print(loss)\n",
    "\n",
    "            # Log the loss to TensorBoard\n",
    "            writer.add_scalar('Training Loss', loss.item(), epoch * len(dataloader) + i)\n",
    "\n",
    "            # Backpropagate loss and update parameters\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # Clip gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            # Update learning rate\n",
    "            scheduler.step()\n",
    "\n",
    "            # Save model parameters every 500 iterations\n",
    "            if (i + 1) % 50 == 0:\n",
    "                torch.save(model.state_dict(), os.path.join(output_dir, f'{output_prefix}_epoch_{epoch}_iter_{i + 1}.pt'))\n",
    "\n",
    "        print(f'Epoch: {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "        # Save model checkpoint at the end of each epoch\n",
    "        torch.save(model.state_dict(), os.path.join(output_dir, f'{output_prefix}_epoch_{epoch}.pt'))\n",
    "\n",
    "    # Load the best model checkpoint\n",
    "    model.load_state_dict(torch.load(os.path.join(output_dir, f'{output_prefix}_best.pt')))\n",
    "\n",
    "    # Close the SummaryWriter\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dhavi\\anaconda3\\envs\\clipcap\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "3.5\n",
      "3.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B\n",
      "id isDSWBA\n",
      "C\n",
      "D\n",
      "E\n",
      "F\n",
      "G\n",
      "3.8\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TransformerDecoder.forward() missing 1 required positional argument: 'tgt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Create an instance of the model, passing gpt2_model and gpt2_tokenizer as arguments\u001b[39;00m\n\u001b[0;32m      2\u001b[0m model \u001b[39m=\u001b[39m MyModel(d_model, num_heads, dim_feedforward, dropout, num_layers, output_dim)\n\u001b[1;32m----> 4\u001b[0m Train(dataset, model)\n",
      "Cell \u001b[1;32mIn[13], line 55\u001b[0m, in \u001b[0;36mTrain\u001b[1;34m(dataset, model, lr, warmup_steps, output_dir, output_prefix)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m4\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     54\u001b[0m \u001b[39m# Forward pass through your model\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m output \u001b[39m=\u001b[39m model(input_data)\n\u001b[0;32m     57\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m5\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     60\u001b[0m \u001b[39m# Forward pass through GPT-2 model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dhavi\\anaconda3\\envs\\clipcap\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[12], line 10\u001b[0m, in \u001b[0;36mMyModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m      9\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositional_encoding(x)\n\u001b[1;32m---> 10\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(x)\n\u001b[0;32m     11\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp(x)\n\u001b[0;32m     12\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\dhavi\\anaconda3\\envs\\clipcap\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;31mTypeError\u001b[0m: TransformerDecoder.forward() missing 1 required positional argument: 'tgt'"
     ]
    }
   ],
   "source": [
    "# Create an instance of the model, passing gpt2_model and gpt2_tokenizer as arguments\n",
    "model = MyModel(d_model, num_heads, dim_feedforward, dropout, num_layers, output_dim)\n",
    "\n",
    "Train(dataset, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7983\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset.error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([93, 512])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.load(\"C:/Users/dhavi/Downloads/VIdeoCLIPCap/video_tensors_120/01O27.pt\")\n",
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "# def Train(dataset: \"Add dataset\", model: MyModel, args,\n",
    "#                lr: float = 2e-5, warmup_steps: int = 5000, output_dir: str = \".\", output_prefix: str = \"\"):\n",
    "\n",
    "#     num_epochs = 10\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()\n",
    "#         for i, (input_data, target) in enumerate(tqdm(train_loader)):\n",
    "#             input_data, target = input_data.to(device), target.to(device)\n",
    "\n",
    "#             output = model(input_data)\n",
    "\n",
    "#             tokens = gpt2_tokenizer(output.tolist(), return_tensors='pt').to(device)\n",
    "\n",
    "#             gpt2_output = gpt2_model(tokens).last_hidden_state\n",
    "\n",
    "#             gpt2_output = gpt2_output[:, 32:]\n",
    "\n",
    "#             loss = loss_fn(gpt2_output, target)\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "#             optimizer.step()\n",
    "\n",
    "#         print(f'Epoch: {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "#         torch.save(model.state_dict(), os.path.join(output_dir, f'{output_prefix}_epoch_{epoch}.pt'))\n",
    "\n",
    "#     model.load_state_dict(torch.load(os.path.join(output_dir, f'{output_prefix}_best.pt')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(dataset: ClipCocoDataset, model: ClipCaptionModel, args,\n",
    "#           lr: float = 2e-5, warmup_steps: int = 5000, output_dir: str = \".\", output_prefix: str = \"\"):\n",
    "\n",
    "#     device = torch.device('cuda:0')\n",
    "#     batch_size = args.bs\n",
    "#     epochs = args.epochs\n",
    "#     if not os.path.exists(output_dir):\n",
    "#         os.makedirs(output_dir)\n",
    "#     model = model.to(device)\n",
    "#     model.train()\n",
    "#     optimizer = AdamW(model.parameters(), lr=lr)\n",
    "#     train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "#     scheduler = get_linear_schedule_with_warmup(\n",
    "#         optimizer, num_warmup_steps=warmup_steps, num_training_steps=epochs * len(train_dataloader)\n",
    "#     )\n",
    "#     # save_config(args)\n",
    "#     for epoch in range(epochs):\n",
    "#         print(f\">>> Training epoch {epoch}\")\n",
    "#         sys.stdout.flush()\n",
    "#         progress = tqdm(total=len(train_dataloader), desc=output_prefix)\n",
    "#         for idx, (tokens, mask, prefix) in enumerate(train_dataloader):\n",
    "#             model.zero_grad()\n",
    "#             tokens, mask, prefix = tokens.to(device), mask.to(device), prefix.to(device, dtype=torch.float32)\n",
    "#             outputs = model(tokens, prefix, mask)\n",
    "#             logits = outputs.logits[:, dataset.prefix_length - 1: -1]\n",
    "#             loss = nnf.cross_entropy(logits.reshape(-1, logits.shape[-1]), tokens.flatten(), ignore_index=0)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             scheduler.step()\n",
    "#             optimizer.zero_grad()\n",
    "#             progress.set_postfix({\"loss\": loss.item()})\n",
    "#             progress.update()\n",
    "#             if (idx + 1) % 10000 == 0:\n",
    "#                 torch.save(\n",
    "#                     model.state_dict(),\n",
    "#                     os.path.join(output_dir, f\"{output_prefix}_latest.pt\"),\n",
    "#                 )\n",
    "#         progress.close()\n",
    "#         if epoch % args.save_every == 0 or epoch == epochs - 1:\n",
    "#             torch.save(\n",
    "#                 model.state_dict(),\n",
    "#                 os.path.join(output_dir, f\"{output_prefix}-{epoch:03d}.pt\"),\n",
    "#             )\n",
    "#     return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clipcap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
