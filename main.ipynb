{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn.functional as nnf\n",
    "import sys\n",
    "from typing import Tuple, List, Union, Optional\n",
    "\n",
    "import clip\n",
    "import os\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import skimage.io as io\n",
    "import PIL.Image\n",
    "from IPython.display import Image \n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@Data loaders\n",
    "class ClipCocoDataset(Dataset):\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.captions_tokens)\n",
    "\n",
    "    def pad_tokens(self, item: int):\n",
    "        tokens = self.captions_tokens[item]\n",
    "        padding = self.max_seq_len - tokens.shape[0]\n",
    "        if padding > 0:\n",
    "            tokens = torch.cat((tokens, torch.zeros(padding, dtype=torch.int64) - 1))\n",
    "            self.captions_tokens[item] = tokens\n",
    "        elif padding < 0:\n",
    "            tokens = tokens[:self.max_seq_len]\n",
    "            self.captions_tokens[item] = tokens\n",
    "        mask = tokens.ge(0)  # mask is zero where we out of sequence\n",
    "        tokens[~mask] = 0\n",
    "        mask = mask.float()\n",
    "        mask = torch.cat((torch.ones(self.prefix_length), mask), dim=0)  # adding prefix mask\n",
    "        return tokens, mask\n",
    "\n",
    "    def __getitem__(self, item: int) -> Tuple[torch.Tensor, ...]:\n",
    "        tokens, mask = self.pad_tokens(item)\n",
    "        prefix = self.prefixes[self.caption2embedding[item]]\n",
    "        if self.normalize_prefix:\n",
    "            prefix = prefix.float()\n",
    "            prefix = prefix / prefix.norm(2, -1)\n",
    "        return tokens, mask, prefix\n",
    "\n",
    "    def __init__(self, data_path: str,  prefix_length: int, gpt2_type: str = \"gpt2\",\n",
    "                 normalize_prefix=False):\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type)\n",
    "        self.prefix_length = prefix_length\n",
    "        self.normalize_prefix = normalize_prefix\n",
    "        with open(data_path, 'rb') as f:\n",
    "            all_data = pickle.load(f)\n",
    "        print(\"Data size is %0d\" % len(all_data[\"clip_embedding\"]))\n",
    "        sys.stdout.flush()\n",
    "        self.prefixes = all_data[\"clip_embedding\"]\n",
    "        captions_raw = all_data[\"captions\"]\n",
    "        self.image_ids = [caption[\"image_id\"] for caption in captions_raw]\n",
    "        self.captions = [caption['caption'] for caption in captions_raw]\n",
    "        if os.path.isfile(f\"{data_path[:-4]}_tokens.pkl\"):\n",
    "            with open(f\"{data_path[:-4]}_tokens.pkl\", 'rb') as f:\n",
    "                self.captions_tokens, self.caption2embedding, self.max_seq_len = pickle.load(f)\n",
    "        else:\n",
    "            self.captions_tokens = []\n",
    "            self.caption2embedding = []\n",
    "            max_seq_len = 0\n",
    "            for caption in captions_raw:\n",
    "                self.captions_tokens.append(torch.tensor(self.tokenizer.encode(caption['caption']), dtype=torch.int64))\n",
    "                self.caption2embedding.append(caption[\"clip_embedding\"])\n",
    "                max_seq_len = max(max_seq_len, self.captions_tokens[-1].shape[0])\n",
    "            with open(f\"{data_path[:-4]}_tokens.pkl\", 'wb') as f:\n",
    "                pickle.dump([self.captions_tokens, self.caption2embedding, max_seq_len], f)\n",
    "        all_len = torch.tensor([len(self.captions_tokens[i]) for i in range(len(self))]).float()\n",
    "        self.max_seq_len = min(int(all_len.mean() + all_len.std() * 10), int(all_len.max()))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipCocoDataset(Dataset):\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.captions_tokens)\n",
    "\n",
    "    def pad_tokens(self, item: int):\n",
    "        tokens = self.captions_tokens[item]\n",
    "        padding = self.max_seq_len - tokens.shape[0]\n",
    "        if padding > 0:\n",
    "            tokens = torch.cat((tokens, torch.zeros(padding, dtype=torch.int64) - 1))\n",
    "            self.captions_tokens[item] = tokens\n",
    "        elif padding < 0:\n",
    "            tokens = tokens[:self.max_seq_len]\n",
    "            self.captions_tokens[item] = tokens\n",
    "        mask = tokens.ge(0)  # mask is zero where we out of sequence\n",
    "        tokens[~mask] = 0\n",
    "        mask = mask.float()\n",
    "        mask = torch.cat((torch.ones(self.prefix_length), mask), dim=0)  # adding prefix mask\n",
    "        return tokens, mask\n",
    "\n",
    "    def __getitem__(self, item: int) -> Tuple[torch.Tensor, ...]:\n",
    "        tokens, mask = self.pad_tokens(item)\n",
    "        prefix = self.prefixes[self.caption2embedding[item]]\n",
    "        if self.normalize_prefix:\n",
    "            prefix = prefix.float()\n",
    "            prefix = prefix / prefix.norm(2, -1)\n",
    "        return tokens, mask, prefix\n",
    "\n",
    "    def __init__(self, data_path: str,  prefix_length: int, gpt2_type: str = \"gpt2\",\n",
    "                 normalize_prefix=False):\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type)\n",
    "        self.prefix_length = prefix_length\n",
    "        self.normalize_prefix = normalize_prefix\n",
    "        with open(data_path, 'rb') as f:\n",
    "            all_data = pickle.load(f)\n",
    "        print(\"Data size is %0d\" % len(all_data[\"clip_embedding\"]))\n",
    "        sys.stdout.flush()\n",
    "        self.prefixes = all_data[\"clip_embedding\"]\n",
    "        captions_raw = all_data[\"captions\"]\n",
    "        self.image_ids = [caption[\"image_id\"] for caption in captions_raw]\n",
    "        self.captions = [caption['caption'] for caption in captions_raw]\n",
    "        if os.path.isfile(f\"{data_path[:-4]}_tokens.pkl\"):\n",
    "            with open(f\"{data_path[:-4]}_tokens.pkl\", 'rb') as f:\n",
    "                self.captions_tokens, self.caption2embedding, self.max_seq_len = pickle.load(f)\n",
    "        else:\n",
    "            self.captions_tokens = []\n",
    "            self.caption2embedding = []\n",
    "            max_seq_len = 0\n",
    "            for caption in captions_raw:\n",
    "                self.captions_tokens.append(torch.tensor(self.tokenizer.encode(caption['caption']), dtype=torch.int64))\n",
    "                self.caption2embedding.append(caption[\"clip_embedding\"])\n",
    "                max_seq_len = max(max_seq_len, self.captions_tokens[-1].shape[0])\n",
    "            # self.max_seq_len = max_seq_len\n",
    "            with open(f\"{data_path[:-4]}_tokens.pkl\", 'wb') as f:\n",
    "                pickle.dump([self.captions_tokens, self.caption2embedding, max_seq_len], f)\n",
    "        all_len = torch.tensor([len(self.captions_tokens[i]) for i in range(len(self))]).float()\n",
    "        self.max_seq_len = min(int(all_len.mean() + all_len.std() * 10), int(all_len.max()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MappingType(Enum):\n",
    "    Transformer = 'transformer'\n",
    "\n",
    "class MlpTransformer(nn.Module):\n",
    "    def __init__(self, in_dim, h_dim, out_d: Optional[int] = None, act=nnf.relu, dropout=0.):\n",
    "        super().__init__()\n",
    "        out_d = out_d if out_d is not None else in_dim\n",
    "        self.fc1 = nn.Linear(in_dim, h_dim)\n",
    "        self.act = act\n",
    "        self.fc2 = nn.Linear(h_dim, out_d)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_self, dim_ref, num_heads, bias=True, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim_self // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "        self.to_queries = nn.Linear(dim_self, dim_self, bias=bias)\n",
    "        self.to_keys_values = nn.Linear(dim_ref, dim_self * 2, bias=bias)\n",
    "        self.project = nn.Linear(dim_self, dim_self)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, y=None, mask=None):\n",
    "        y = y if y is not None else x\n",
    "        b, n, c = x.shape\n",
    "        _, m, d = y.shape\n",
    "        queries = self.to_queries(x).reshape(b, n, self.num_heads, c // self.num_heads)\n",
    "        keys_values = self.to_keys_values(y).reshape(b, m, 2, self.num_heads, c // self.num_heads)\n",
    "        keys, values = keys_values[:, :, 0], keys_values[:, :, 1]\n",
    "        attention = torch.einsum('bnhd,bmhd->bnmh', queries, keys) * self.scale\n",
    "        if mask is not None:\n",
    "            if mask.dim() == 2:\n",
    "                mask = mask.unsqueeze(1)\n",
    "            attention = attention.masked_fill(mask.unsqueeze(3), float(\"-inf\"))\n",
    "        attention = attention.softmax(dim=2)\n",
    "        out = torch.einsum('bnmh,bmhd->bnhd', attention, values).reshape(b, n, c)\n",
    "        out = self.project(out)\n",
    "        return out, attention\n",
    "\n",
    "\n",
    "class TransformerLayer(nn.Module):\n",
    "\n",
    "    def forward_with_attention(self, x, y=None, mask=None):\n",
    "        x_, attention = self.attn(self.norm1(x), y, mask)\n",
    "        x = x + x_\n",
    "        x = x + self.norm2(x)\n",
    "        return x, attention\n",
    "\n",
    "    def forward(self, x, y=None, mask=None):\n",
    "        x = x + self.attn(self.norm1(x), y, mask)[0]\n",
    "        x = x + self.norm2(x)\n",
    "        return x\n",
    "\n",
    "    def __init__(self, dim_self, dim_ref, num_heads, mlp_ratio=4., bias=False, dropout=0., act=nnf.relu,\n",
    "                 norm_layer: nn.Module = nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim_self)\n",
    "        self.attn = MultiHeadAttention(dim_self, dim_ref, num_heads, bias=bias, dropout=dropout)\n",
    "        self.norm2 = norm_layer(dim_self)\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    def forward_with_attention(self, x, y=None, mask=None):\n",
    "        attentions = []\n",
    "        for layer in self.layers:\n",
    "            x, att = layer.forward_with_attention(x, y, mask)\n",
    "            attentions.append(att)\n",
    "        return x, attentions\n",
    "\n",
    "    def forward(self, x, y=None, mask=None):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if i % 2 == 0 and self.enc_dec: \n",
    "                x = layer(x, y)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def __init__(self, dim, depth, num_heads, mlp_ratio=4., bias=False, dropout=0., norm_layer: nn.Module = nn.LayerNorm,\n",
    "                 enc_dec=False):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(TransformerLayer(dim, dim, num_heads, mlp_ratio, bias=bias, dropout=dropout,\n",
    "                                                norm_layer=norm_layer))\n",
    "        self.enc_dec = enc_dec\n",
    "\n",
    "\n",
    "class TransformerMapper(nn.Module):\n",
    "\n",
    "    def forward(self, prefix):\n",
    "        return self.transformer(prefix)\n",
    "\n",
    "    def __init__(self, dim, prefix_length, depth, num_heads, mlp_ratio=4., bias=False, dropout=0., act=nnf.relu,\n",
    "                 norm_layer: nn.Module = nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.transformer = Transformer(dim, depth, num_heads, mlp_ratio, bias=bias, dropout=dropout,\n",
    "                                       norm_layer=norm_layer)\n",
    "        self.linear = nn.Linear(dim, dim)\n",
    "\n",
    "\n",
    "class ClipCaptionModel(nn.Module):\n",
    "\n",
    "    def __init__(self, mapping_type: MappingType, num_hidden_layers: int, dropout: float, **kwargs):\n",
    "        super().__init__()\n",
    "        if mapping_type == MappingType.Transformer:\n",
    "            self.mapper = TransformerMapper(kwargs['clip_dim'], kwargs['prefix_length'], num_hidden_layers, \n",
    "                                            kwargs['num_attention_heads'], kwargs['intermediate_size'], \n",
    "                                            kwargs['hidden_act'], kwargs['layer_norm_eps'])\n",
    "        else:\n",
    "            raise ValueError(f\"Mapping type {mapping_type} not recognized.\")\n",
    "\n",
    "    def forward(self, prefix):\n",
    "        return self.mapper(prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA = get_device\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "save_path = os.path.join(os.path.dirname(current_directory), \"pretrained_models\")\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "model_path = os.path.join(save_path, 'model_wieghts.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "###Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load the CLIP model and GPT-2 tokenizer\n",
    "clip_model, preprocess = clip.load(\"RN50x4\", device=\"cuda\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Define the Transformer model\n",
    "class VideoTransformer(nn.Module):\n",
    "    def __init__(self, clip_dim, time_dim, hidden_dim, nhead, num_layers):\n",
    "        super().__init__()\n",
    "        self.clip_dim = clip_dim\n",
    "        self.time_dim = time_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.nhead = nhead\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Define the input linear layer\n",
    "        self.input_linear = nn.Linear(clip_dim + time_dim, hidden_dim)\n",
    "\n",
    "        # Define the Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(hidden_dim, nhead)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "\n",
    "        # Define the output linear layer\n",
    "        self.output_linear = nn.Linear(hidden_dim, tokenizer.vocab_size)\n",
    "\n",
    "    def forward(self, clip_features, time_features):\n",
    "        # Concatenate the CLIP features and time features\n",
    "        x = torch.cat((clip_features, time_features), dim=-1)\n",
    "\n",
    "        # Pass the input through the input linear layer\n",
    "        x = self.input_linear(x)\n",
    "\n",
    "        # Pass the input through the Transformer encoder\n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        # Pass the output through the output linear layer\n",
    "        x = self.output_linear(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Instantiate the Transformer model\n",
    "model = VideoTransformer(clip_model.visual.output_dim, 1, 512, 8, 6)\n",
    "\n",
    "# Load a video and extract frames\n",
    "video_frames = load_video_frames(\"my_video.mp4\")\n",
    "\n",
    "# Process each frame using CLIP and the Transformer model\n",
    "for i, frame in enumerate(video_frames):\n",
    "    # Preprocess the frame using CLIP's preprocess function\n",
    "    frame = preprocess(frame)\n",
    "\n",
    "    # Compute the CLIP features for the frame\n",
    "    with torch.no_grad():\n",
    "        clip_features = clip_model.encode_image(frame)\n",
    "\n",
    "    # Compute the time feature for the frame (frame number divided by frame rate)\n",
    "    time_feature = torch.tensor([[i / FRAME_RATE]])\n",
    "\n",
    "    # Pass the CLIP features and time feature through the Transformer model\n",
    "    output_token_logits = model(clip_features.unsqueeze(0), time_feature.unsqueeze(0))\n",
    "\n",
    "    # Compute the most likely output token\n",
    "    output_token = torch.argmax(output_token_logits, dim=-1).item()\n",
    "\n",
    "    # Decode the output token using GPT-2's decode function\n",
    "    output_text = tokenizer.decode(output_token)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clipcap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
