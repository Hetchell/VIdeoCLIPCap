{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn.functional as nnf\n",
    "import sys\n",
    "from typing import Tuple, List, Union, Optional\n",
    "\n",
    "import clip\n",
    "import os\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import skimage.io as io\n",
    "import PIL.Image\n",
    "from IPython.display import Image \n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@Data loaders\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA = get_device\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "save_path = os.path.join(os.path.dirname(current_directory), \"pretrained_models\")\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "model_path = os.path.join(save_path, 'model_wieghts.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_self, dim_ref, num_heads, bias=True, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim_self // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "        self.to_queries = nn.Linear(dim_self, dim_self, bias=bias)\n",
    "        self.to_keys_values = nn.Linear(dim_ref, dim_self * 2, bias=bias)\n",
    "        self.project = nn.Linear(dim_self, dim_self)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, y=None, mask=None):\n",
    "        y = y if y is not None else x\n",
    "        b, n, c = x.shape\n",
    "        _, m, d = y.shape\n",
    "        queries = self.to_queries(x).reshape(b, n, self.num_heads, c // self.num_heads)\n",
    "        keys_values = self.to_keys_values(y).reshape(b, m, 2, self.num_heads, c // self.num_heads)\n",
    "        keys, values = keys_values[:, :, 0], keys_values[:, :, 1]\n",
    "        attention = torch.einsum('bnhd,bmhd->bnmh', queries, keys) * self.scale\n",
    "        if mask is not None:\n",
    "            if mask.dim() == 2:\n",
    "                mask = mask.unsqueeze(1)\n",
    "            attention = attention.masked_fill(mask.unsqueeze(3), float(\"-inf\"))\n",
    "        attention = attention.softmax(dim=2)\n",
    "        out = torch.einsum('bnmh,bmhd->bnhd', attention, values).reshape(b, n, c)\n",
    "        out = self.project(out)\n",
    "        return out, attention\n",
    "\n",
    "\n",
    "class TransformerLayer(nn.Module):\n",
    "    def forward_with_attention(self, x, y=None, mask=None):\n",
    "        x_, attention = self.attn(self.norm1(x), y, mask)\n",
    "        x = x + x_\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x, attention\n",
    "\n",
    "    def forward(self, x, y=None, mask=None):\n",
    "        x = x + self.attn(self.norm1(x), y, mask)[0]\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "    def __init__(self, dim_self, dim_ref, num_heads, mlp_ratio=4., bias=False, dropout=0., act=nn.ReLU,\n",
    "                 norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim_self)\n",
    "        self.attn = MultiHeadAttention(dim_self, dim_ref, num_heads, bias=bias, dropout=dropout)\n",
    "        self.norm2 = norm_layer(dim_self)\n",
    "        self.mlp = MlpTransformer(dim_self, int(dim_self * mlp_ratio), act=act, dropout=dropout)\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def forward_with_attention(self, x, y=None, mask=None):\n",
    "        attentions = []\n",
    "        for layer in self.layers:\n",
    "            x, att = layer.forward_with_attention(x, y, mask)\n",
    "            attentions.append(att)\n",
    "        return x, attentions\n",
    "\n",
    "    def forward(self, x, y=None, mask=None):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if i % 2 == 0 and self.enc_dec: # cross\n",
    "                x = layer(x, y)\n",
    "            elif self.enc_dec:  # self\n",
    "                x = layer(x, x, mask)\n",
    "            else:  # self or cross\n",
    "                x = layer(x, y, mask)\n",
    "        return x\n",
    "\n",
    "    def __init__(self, dim_self: int, num_heads: int, num_layers: int, dim_ref: int = None,\n",
    "                 mlp_ratio: float = 2., act=nn.ReLU, norm_layer=nn.LayerNorm, enc_dec=False):\n",
    "        super(Transformer, self).__init__()\n",
    "        dim_ref = dim_ref if dim_ref is not None else dim_self\n",
    "        self.enc_dec = enc_dec\n",
    "        if enc_dec:\n",
    "            num_layers = num_layers * 2\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            if i % 2 == 0 and enc_dec:  # cross\n",
    "                layers.append(TransformerLayer(dim_self, dim_ref, num_heads, mlp_ratio, act=act, norm_layer=norm_layer))\n",
    "            elif enc_dec:  # self\n",
    "                layers.append(TransformerLayer(dim_self, dim_self, num_heads, mlp_ratio, act=act, norm_layer=norm_layer))\n",
    "            else:  # self or cross\n",
    "                layers.append(TransformerLayer(dim_self, dim_ref, num_heads, mlp_ratio, act=act, norm_layer=norm_layer))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "\n",
    "class TransformerMapper(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x).view(x.shape[0], self.clip_length, -1)\n",
    "        prefix = self.prefix_const.unsqueeze(0).expand(x.shape[0], *self.prefix_const.shape)\n",
    "        prefix = torch.cat((x, prefix), dim=1)\n",
    "        out = self.transformer(prefix)[:, self.clip_length:]\n",
    "        return out\n",
    "\n",
    "    def __init__(self, dim_clip: int, dim_embedding: int, prefix_length: int, clip_length: int, num_layers: int = 8):\n",
    "        super(TransformerMapper, self).__init__()\n",
    "        self.clip_length = clip_length\n",
    "        self.transformer = Transformer(dim_embedding, 8, num_layers)\n",
    "        self.linear = nn.Linear(dim_clip, clip_length * dim_embedding)\n",
    "        self.prefix_const = nn.Parameter(torch.randn(prefix_length, dim_embedding), requires_grad=True)\n",
    "\n",
    "\n",
    "class ClipCaptionModel(nn.Module):\n",
    "    def get_dummy_token(self, batch_size: int, device: torch.device) -> torch.Tensor:\n",
    "        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor, prefix: torch.Tensor, mask: torch.Tensor = None,\n",
    "                labels: torch.Tensor = None):\n",
    "        embedding_text = self.gpt.transformer.wte(tokens)\n",
    "        prefix_projections = self.clip_project(prefix).view(-1, self.prefix_length, self.gpt_embedding_size)\n",
    "        embedding_cat = torch.cat((prefix_projections, embedding_text), dim=1)\n",
    "        if labels is not None:\n",
    "            dummy_token = self.get_dummy_token(tokens.shape[0], tokens.device)\n",
    "            labels = torch.cat((dummy_token, tokens), dim=1)\n",
    "        out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=mask)\n",
    "        return out\n",
    "\n",
    "    def __init__(self, prefix_length: int, clip_length: int = None, prefix_size: int = 512,\n",
    "                 num_layers: int = 8, mapping_type: MappingType = MappingType.MLP):\n",
    "        super(ClipCaptionModel, self).__init__()\n",
    "        self.prefix_length = prefix_length\n",
    "        self.gpt = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "        self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n",
    "        if mapping_type == MappingType.MLP:\n",
    "            self.clip_project = MLP((prefix_size, (self.gpt_embedding_size * prefix_length) // 2,\n",
    "                                     self.gpt_embedding_size * prefix_length))\n",
    "        else:\n",
    "            self.clip_project = TransformerMapper(prefix_size, self.gpt_embedding_size, prefix_length,\n",
    "                                                                     clip_length, num_layers)\n",
    "\n",
    "\n",
    "class ClipCaptionPrefix(ClipCaptionModel):\n",
    "    def parameters(self, recurse: bool = True):\n",
    "        return self.clip_project.parameters()\n",
    "\n",
    "    def train(self, mode: bool = True):\n",
    "        super(ClipCaptionPrefix, self).train(mode)\n",
    "        self.gpt.eval()\n",
    "        return self\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clipcap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
