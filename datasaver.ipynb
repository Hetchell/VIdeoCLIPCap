{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import skimage.io as io\n",
    "import clip\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model_type = \"ViT-B/32\"\n",
    "# out_path = \"./data/Video_CLIP_ViT-B_32.pkl\"\n",
    "csv_path = \"./Charades/Charades_v1_train.csv\"\n",
    "video_path = \"./video\" \n",
    "frames_no = 10 \n",
    "num_video_batch = 5\n",
    "model, preprocess = clip.load(clip_model_type, device=device, jit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract frames\n",
    "def extract_frames(video_file):\n",
    "    video, audio, info = torchvision.io.read_video(video_file, output_format='TCHW')\n",
    "\n",
    "    num_frames = len(video)\n",
    "    frame_rate = info[\"video_fps\"]\n",
    "    interval = int(frame_rate * 60 / frames_no)\n",
    "    frames = []\n",
    "    to_pil = torchvision.transforms.ToPILImage()\n",
    "    for i in range(0, num_frames, interval):\n",
    "      frames.append(to_pil(video[i].squeeze(0)))\n",
    "      print(to_pil(video[i].squeeze(0)).shape)\n",
    "    \n",
    "\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode using clip image encoder and stack the tensors alon a new dimension\n",
    "def preprocess_frames(frames):\n",
    "\n",
    "    tensors = []\n",
    "\n",
    "    for i in range(0,len(frames)): #should this be len()+1???\n",
    "        frame = preprocess(frames[i]).unsqueeze(0).to(device)\n",
    "        tensors.append(frame)\n",
    "\n",
    "    preprocess_batch = torch.stack(tensors)\n",
    "    preprocess_batch = preprocess_batch.to(device)\n",
    "\n",
    "    return preprocess_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to concatenate tensors \n",
    "def CLIPtokenise(tensors):\n",
    "    #the input tensors are 3d stacks (lets say x,y,z)\n",
    "    #function returns a tensor of shape (x, y, sum(z)) and a list of z values\n",
    "\n",
    "    for t in tensors:\n",
    "        print(t.shape) \n",
    "\n",
    "    # get the z values for each tensor\n",
    "    z_values = [t.shape[2] for t in tensors]\n",
    "\n",
    "    joined = torch.cat(tensors, dim=2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "            image_features = model.encode_image(joined)\n",
    "\n",
    "    image_features.to(device)\n",
    "\n",
    "    #gets z values and splits them back to video stacks\n",
    "    split = torch.split(image_features, z_values, dim=2)\n",
    "\n",
    "    return split\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dhavi\\anaconda3\\envs\\clipcap\\Lib\\site-packages\\torchvision\\io\\video.py:161: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "shape",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39mfor\u001b[39;00m \u001b[39mid\u001b[39m \u001b[39min\u001b[39;00m batch_ids:\n\u001b[0;32m     16\u001b[0m     \u001b[39m# get the video file name with the corresponding id\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     video_file \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mvideo_path\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mid\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.mp4\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 18\u001b[0m     batch\u001b[39m.\u001b[39mappend(preprocess_frames(extract_frames(video_file)))\n\u001b[0;32m     20\u001b[0m tokenised_batch \u001b[39m=\u001b[39m CLIPtokenise(batch)\n\u001b[0;32m     22\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(batch_ids)):\n",
      "Cell \u001b[1;32mIn[3], line 12\u001b[0m, in \u001b[0;36mextract_frames\u001b[1;34m(video_file)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, num_frames, interval):\n\u001b[0;32m     11\u001b[0m   frames\u001b[39m.\u001b[39mappend(to_pil(video[i]\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)))\n\u001b[1;32m---> 12\u001b[0m   \u001b[39mprint\u001b[39m(to_pil(video[i]\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m))\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     15\u001b[0m \u001b[39mreturn\u001b[39;00m frames\n",
      "File \u001b[1;32mc:\\Users\\dhavi\\anaconda3\\envs\\clipcap\\Lib\\site-packages\\PIL\\Image.py:529\u001b[0m, in \u001b[0;36mImage.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    527\u001b[0m     deprecate(\u001b[39m\"\u001b[39m\u001b[39mImage categories\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m10\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mis_animated\u001b[39m\u001b[39m\"\u001b[39m, plural\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    528\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_category\n\u001b[1;32m--> 529\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(name)\n",
      "\u001b[1;31mAttributeError\u001b[0m: shape"
     ]
    }
   ],
   "source": [
    "# load captions and video names\n",
    "# df = pd.read_csv(csv_path)\n",
    "df_new = pd.read_csv(csv_path)\n",
    "df = df_new.head(15)\n",
    "ids = df[\"id\"]\n",
    "\n",
    "# loop over the ids with a batch size step\n",
    "for i in tqdm(range(0, len(ids), num_video_batch)):\n",
    "    # get the current batch of ids\n",
    "    batch_ids = ids[i:i+num_video_batch]\n",
    "\n",
    "    batch = []\n",
    "\n",
    "    # loop over the batch ids\n",
    "    for id in batch_ids:\n",
    "        # get the video file name with the corresponding id\n",
    "        video_file = f\"{video_path}/{id}.mp4\"\n",
    "        batch.append(preprocess_frames(extract_frames(video_file)))\n",
    "        \n",
    "    tokenised_batch = CLIPtokenise(batch)\n",
    "\n",
    "    for j in range(len(batch_ids)):\n",
    "        torch.save(tokenised_batch[j], f\"./video_tensors/{batch_ids[j]}.pt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clipcap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
