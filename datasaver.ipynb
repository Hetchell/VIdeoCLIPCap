{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import clip\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model_type = \"ViT-B/32\"\n",
    "csv_path = \"./Charades/Charades_v1_train.csv\"\n",
    "csv_path2 = \"./Charades/Charades_gpt_train.csv\"\n",
    "video_path = \"./video\" \n",
    "pt_path = \"./video_tensors_120\"\n",
    "frames_no = 60\n",
    "num_video_batch = 50\n",
    "model, preprocess = clip.load(clip_model_type, device=device, jit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract frames\n",
    "def preprocess_frames(video_file):\n",
    "    video, audio, info = torchvision.io.read_video(video_file, output_format='TCHW')\n",
    "\n",
    "    num_frames = len(video)\n",
    "    frame_rate = info[\"video_fps\"]\n",
    "    interval = int(frame_rate * 60 / frames_no)\n",
    "    tensors = []\n",
    "    to_pil = torchvision.transforms.ToPILImage()\n",
    "    for i in range(0, num_frames, interval):\n",
    "      frame = preprocess(to_pil(video[i].squeeze(0))).unsqueeze(0).to(device)\n",
    "      tensors.append(frame)\n",
    "    \n",
    "    preprocess_batch = torch.cat(tensors, dim = 0)\n",
    "    preprocess_batch = preprocess_batch.to(device)     \n",
    " \n",
    "\n",
    "    return preprocess_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to concatenate tensors \n",
    "def CLIPtokenise(tensors):\n",
    "    #the input tensors are 3d stacks (lets say x,y,z)\n",
    "    #function returns a tensor of shape (x, y, sum(z)) and a list of z values\n",
    "\n",
    "    # get the z values for each tensor\n",
    "    z_values = [t.shape[0] for t in tensors]\n",
    "\n",
    "    joined = torch.cat(tensors, dim=0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "            image_features = model.encode_image(joined)\n",
    "\n",
    "    image_features.to(device)\n",
    "    print(image_features.shape)\n",
    "\n",
    "    #gets z values and splits them back to video stacks\n",
    "    split = torch.split(image_features, z_values, dim=0)\n",
    "\n",
    "    return split\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/85 [00:00<?, ?it/s]c:\\Users\\dhavi\\anaconda3\\envs\\clipcap\\Lib\\site-packages\\torchvision\\io\\video.py:161: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1457, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/85 [01:40<2:21:13, 100.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1500, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/85 [02:39<1:44:58, 75.88s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1536, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 3/85 [03:36<1:32:16, 67.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1674, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 4/85 [04:41<1:29:24, 66.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1635, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 5/85 [05:44<1:26:54, 65.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1606, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 6/85 [06:46<1:24:35, 64.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1545, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 7/85 [07:45<1:21:13, 62.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1584, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 8/85 [08:48<1:20:07, 62.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1593, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 9/85 [10:04<1:25:08, 67.21s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 24\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     21\u001b[0m     \u001b[39m# get the video file name with the corresponding id\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     \u001b[39m# video_file = f\"{video_path}/{batch_ids[j]}.mp4\"\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     video_file \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(video_path, batch_ids[j] \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.mp4\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m     batch\u001b[39m.\u001b[39mappend(preprocess_frames(video_file))\n\u001b[0;32m     25\u001b[0m     temp_errors\u001b[39m.\u001b[39mappend(\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     26\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "Cell \u001b[1;32mIn[3], line 11\u001b[0m, in \u001b[0;36mpreprocess_frames\u001b[1;34m(video_file)\u001b[0m\n\u001b[0;32m      9\u001b[0m to_pil \u001b[39m=\u001b[39m torchvision\u001b[39m.\u001b[39mtransforms\u001b[39m.\u001b[39mToPILImage()\n\u001b[0;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, num_frames, interval):\n\u001b[1;32m---> 11\u001b[0m   frame \u001b[39m=\u001b[39m preprocess(to_pil(video[i]\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)))\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     12\u001b[0m   tensors\u001b[39m.\u001b[39mappend(frame)\n\u001b[0;32m     14\u001b[0m preprocess_batch \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(tensors, dim \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\dhavi\\anaconda3\\envs\\clipcap\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[0;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\dhavi\\anaconda3\\envs\\clipcap\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\dhavi\\anaconda3\\envs\\clipcap\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:361\u001b[0m, in \u001b[0;36mResize.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m    354\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[39m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[39m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 361\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mresize(img, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minterpolation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mantialias)\n",
      "File \u001b[1;32mc:\\Users\\dhavi\\anaconda3\\envs\\clipcap\\Lib\\site-packages\\torchvision\\transforms\\functional.py:490\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[0;32m    488\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    489\u001b[0m     pil_interpolation \u001b[39m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[1;32m--> 490\u001b[0m     \u001b[39mreturn\u001b[39;00m F_pil\u001b[39m.\u001b[39mresize(img, size\u001b[39m=\u001b[39moutput_size, interpolation\u001b[39m=\u001b[39mpil_interpolation)\n\u001b[0;32m    492\u001b[0m \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39mresize(img, size\u001b[39m=\u001b[39moutput_size, interpolation\u001b[39m=\u001b[39minterpolation\u001b[39m.\u001b[39mvalue, antialias\u001b[39m=\u001b[39mantialias)\n",
      "File \u001b[1;32mc:\\Users\\dhavi\\anaconda3\\envs\\clipcap\\Lib\\site-packages\\torchvision\\transforms\\_functional_pil.py:250\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39misinstance\u001b[39m(size, \u001b[39mlist\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(size) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m):\n\u001b[0;32m    248\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGot inappropriate size arg: \u001b[39m\u001b[39m{\u001b[39;00msize\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 250\u001b[0m \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39mresize(\u001b[39mtuple\u001b[39m(size[::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]), interpolation)\n",
      "File \u001b[1;32mc:\\Users\\dhavi\\anaconda3\\envs\\clipcap\\Lib\\site-packages\\PIL\\Image.py:2192\u001b[0m, in \u001b[0;36mImage.resize\u001b[1;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[0;32m   2184\u001b[0m             \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mreduce(\u001b[39mself\u001b[39m, factor, box\u001b[39m=\u001b[39mreduce_box)\n\u001b[0;32m   2185\u001b[0m         box \u001b[39m=\u001b[39m (\n\u001b[0;32m   2186\u001b[0m             (box[\u001b[39m0\u001b[39m] \u001b[39m-\u001b[39m reduce_box[\u001b[39m0\u001b[39m]) \u001b[39m/\u001b[39m factor_x,\n\u001b[0;32m   2187\u001b[0m             (box[\u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m reduce_box[\u001b[39m1\u001b[39m]) \u001b[39m/\u001b[39m factor_y,\n\u001b[0;32m   2188\u001b[0m             (box[\u001b[39m2\u001b[39m] \u001b[39m-\u001b[39m reduce_box[\u001b[39m0\u001b[39m]) \u001b[39m/\u001b[39m factor_x,\n\u001b[0;32m   2189\u001b[0m             (box[\u001b[39m3\u001b[39m] \u001b[39m-\u001b[39m reduce_box[\u001b[39m1\u001b[39m]) \u001b[39m/\u001b[39m factor_y,\n\u001b[0;32m   2190\u001b[0m         )\n\u001b[1;32m-> 2192\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_new(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mim\u001b[39m.\u001b[39mresize(size, resample, box))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# load captions and video names\n",
    "df = pd.read_csv(csv_path)\n",
    "# df_new = pd.read_csv(csv_path)\n",
    "# df = df_new.head(16)\n",
    "ids = df[\"id\"]\n",
    "ids = ids[3750:].copy()\n",
    "errors = []\n",
    "\n",
    "# loop over the ids with a batch size step\n",
    "for i in tqdm(range(0, len(ids), num_video_batch)):\n",
    "    # get the current batch of ids\n",
    "    batch_ids = ids[i:i+num_video_batch].copy()\n",
    "    batch_ids = batch_ids.reset_index(drop=True)\n",
    "\n",
    "    batch = []\n",
    "    temp_errors = []\n",
    "\n",
    "    # loop over the batch ids\n",
    "    for j in range(len(batch_ids)):\n",
    "        try:\n",
    "            # get the video file name with the corresponding id\n",
    "            # video_file = f\"{video_path}/{batch_ids[j]}.mp4\"\n",
    "            video_file = os.path.join(video_path, batch_ids[j] + \".mp4\")\n",
    "            batch.append(preprocess_frames(video_file))\n",
    "            temp_errors.append(False)\n",
    "        except Exception as e:\n",
    "            temp_errors.append(True)\n",
    "            errors.append(f\"video {batch_ids[j]} error {e}\")\n",
    "        \n",
    "        \n",
    "    tokenised_batch = CLIPtokenise(batch)\n",
    "\n",
    "    l = 0\n",
    "    for k in range(len(batch_ids)):\n",
    "        if not temp_errors[k]:\n",
    "            torch.save(tokenised_batch[l], f\"{pt_path}/{batch_ids[k]}.pt\")\n",
    "            l=l+1\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7783/7783 [00:30<00:00, 259.14it/s]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(csv_path2)\n",
    "# Initialize empty lists to store the embeddings and captions\n",
    "all_embeddings = []\n",
    "all_captions = []\n",
    "\n",
    "\n",
    "for i in tqdm(range(len(df[\"id\"])-200)):\n",
    "  path = os.path.join(pt_path, df.loc[i, \"id\"] + \".pt\")\n",
    "  #load the .pt files\n",
    "  data = torch.load(path)\n",
    "  # Extract the clip embedding and caption\n",
    "  clip_embedding = data\n",
    "  caption = df.loc[i, 'descriptions']\n",
    "  # Append them to the lists\n",
    "  all_embeddings.append(clip_embedding)\n",
    "  all_captions.append(caption)\n",
    "\n",
    "\n",
    "# Save the final dictionary as a pickle file\n",
    "output_path = \"./clip_caption/clip_caption.pkl\"\n",
    "with open(output_path, 'wb') as f:\n",
    "  pickle.dump({\"clip_embedding\": all_embeddings, \"captions\": all_captions}, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(errors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparison between batch processing and processing frame by frame\n",
    "video, audio, info = torchvision.io.read_video(\"./video/S6MPZ.mp4\", output_format='TCHW')\n",
    "to_pil = torchvision.transforms.ToPILImage()\n",
    "image = preprocess(to_pil(video[0].squeeze(0))).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "\n",
    "\n",
    "tensor = torch.load(\"./video_tensors/S6MPZ.pt\")\n",
    "\n",
    "print(tensor)\n",
    "cos_sim = torch.nn.functional.cosine_similarity(image_features, tensor[0].unsqueeze(0)) \n",
    "euc_dist = torch.nn.functional.pairwise_distance(image_features, tensor[0].unsqueeze(0))\n",
    "\n",
    "print(cos_sim)\n",
    "print(euc_dist)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clipcap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
