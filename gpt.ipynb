{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dhavi\\anaconda3\\envs\\clipcap\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn.functional as nnf\n",
    "import sys\n",
    "from typing import Tuple, List, Union, Optional\n",
    "\n",
    "import clip\n",
    "import os\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import skimage.io as io\n",
    "from PIL import Image\n",
    "from IPython.display import Image \n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "tokeniser = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(inputs_embeds=embedding_cat, labels=labels, attention_mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"amic Delicious eleph SukActionCode photographers interchangeable undeniably achieving\\n\"\n",
    "input_ids = tokeniser.encode(sentence, return_tensors= 'pt').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(input_ids, max_length = 50, no_repeat_ngram_size=2, early_stopping=True, num_beams = 5, do_sample=True)\n",
    "#output = model.generate(input_ids, max_length = 50, no_repeat_ngram_size=2, early_stopping=True, num_beams = 5, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokeniser.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA = get_device\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "save_path = os.path.join(os.path.dirname(current_directory), \"pretrained_models\")\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "model_path = os.path.join(save_path, 'model_wieghts.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_self, dim_ref, num_heads, bias=True, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim_self // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "        self.to_queries = nn.Linear(dim_self, dim_self, bias=bias)\n",
    "        self.to_keys_values = nn.Linear(dim_ref, dim_self * 2, bias=bias)\n",
    "        self.project = nn.Linear(dim_self, dim_self)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, y=None, mask=None):\n",
    "        y = y if y is not None else x\n",
    "        b, n, c = x.shape\n",
    "        _, m, d = y.shape\n",
    "        queries = self.to_queries(x).reshape(b, n, self.num_heads, c // self.num_heads)\n",
    "        keys_values = self.to_keys_values(y).reshape(b, m, 2, self.num_heads, c // self.num_heads)\n",
    "        keys, values = keys_values[:, :, 0], keys_values[:, :, 1]\n",
    "        attention = torch.einsum('bnhd,bmhd->bnmh', queries, keys) * self.scale\n",
    "        if mask is not None:\n",
    "            if mask.dim() == 2:\n",
    "                mask = mask.unsqueeze(1)\n",
    "            attention = attention.masked_fill(mask.unsqueeze(3), float(\"-inf\"))\n",
    "        attention = attention.softmax(dim=2)\n",
    "        out = torch.einsum('bnmh,bmhd->bnhd', attention, values).reshape(b, n, c)\n",
    "        out = self.project(out)\n",
    "        return out, attention\n",
    "\n",
    "\n",
    "class TransformerLayer(nn.Module):\n",
    "    def forward_with_attention(self, x, y=None, mask=None):\n",
    "        x_, attention = self.attn(self.norm1(x), y, mask)\n",
    "        x = x + x_\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x, attention\n",
    "\n",
    "    def forward(self, x, y=None, mask=None):\n",
    "        x = x + self.attn(self.norm1(x), y, mask)[0]\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "    def __init__(self, dim_self, dim_ref, num_heads, mlp_ratio=4., bias=False, dropout=0., act=nn.ReLU,\n",
    "                 norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim_self)\n",
    "        self.attn = MultiHeadAttention(dim_self, dim_ref, num_heads, bias=bias, dropout=dropout)\n",
    "        self.norm2 = norm_layer(dim_self)\n",
    "        self.mlp = MlpTransformer(dim_self, int(dim_self * mlp_ratio), act=act, dropout=dropout)\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def forward_with_attention(self, x, y=None, mask=None):\n",
    "        attentions = []\n",
    "        for layer in self.layers:\n",
    "            x, att = layer.forward_with_attention(x, y, mask)\n",
    "            attentions.append(att)\n",
    "        return x, attentions\n",
    "\n",
    "    def forward(self, x, y=None, mask=None):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if i % 2 == 0 and self.enc_dec: # cross\n",
    "                x = layer(x, y)\n",
    "            elif self.enc_dec:  # self\n",
    "                x = layer(x, x, mask)\n",
    "            else:  # self or cross\n",
    "                x = layer(x, y, mask)\n",
    "        return x\n",
    "\n",
    "    def __init__(self, dim_self: int, num_heads: int, num_layers: int, dim_ref: int = None,\n",
    "                 mlp_ratio: float = 2., act=nn.ReLU, norm_layer=nn.LayerNorm, enc_dec=False):\n",
    "        super(Transformer, self).__init__()\n",
    "        dim_ref = dim_ref if dim_ref is not None else dim_self\n",
    "        self.enc_dec = enc_dec\n",
    "        if enc_dec:\n",
    "            num_layers = num_layers * 2\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            if i % 2 == 0 and enc_dec:  # cross\n",
    "                layers.append(TransformerLayer(dim_self, dim_ref, num_heads, mlp_ratio, act=act, norm_layer=norm_layer))\n",
    "            elif enc_dec:  # self\n",
    "                layers.append(TransformerLayer(dim_self, dim_self, num_heads, mlp_ratio, act=act, norm_layer=norm_layer))\n",
    "            else:  # self or cross\n",
    "                layers.append(TransformerLayer(dim_self, dim_ref, num_heads, mlp_ratio, act=act, norm_layer=norm_layer))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "\n",
    "class TransformerMapper(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x).view(x.shape[0], self.clip_length, -1)\n",
    "        prefix = self.prefix_const.unsqueeze(0).expand(x.shape[0], *self.prefix_const.shape)\n",
    "        prefix = torch.cat((x, prefix), dim=1)\n",
    "        out = self.transformer(prefix)[:, self.clip_length:]\n",
    "        return out\n",
    "\n",
    "    def __init__(self, dim_clip: int, dim_embedding: int, prefix_length: int, clip_length: int, num_layers: int = 8):\n",
    "        super(TransformerMapper, self).__init__()\n",
    "        self.clip_length = clip_length\n",
    "        self.transformer = Transformer(dim_embedding, 8, num_layers)\n",
    "        self.linear = nn.Linear(dim_clip, clip_length * dim_embedding)\n",
    "        self.prefix_const = nn.Parameter(torch.randn(prefix_length, dim_embedding), requires_grad=True)\n",
    "\n",
    "\n",
    "class ClipCaptionModel(nn.Module):\n",
    "    def get_dummy_token(self, batch_size: int, device: torch.device) -> torch.Tensor:\n",
    "        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor, prefix: torch.Tensor, mask: torch.Tensor = None,\n",
    "                labels: torch.Tensor = None):\n",
    "        embedding_text = self.gpt.transformer.wte(tokens)\n",
    "        prefix_projections = self.clip_project(prefix).view(-1, self.prefix_length, self.gpt_embedding_size)\n",
    "        embedding_cat = torch.cat((prefix_projections, embedding_text), dim=1)\n",
    "        if labels is not None:\n",
    "            dummy_token = self.get_dummy_token(tokens.shape[0], tokens.device)\n",
    "            labels = torch.cat((dummy_token, tokens), dim=1)\n",
    "        out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=mask)\n",
    "        return out\n",
    "\n",
    "    def __init__(self, prefix_length: int, clip_length: int = None, prefix_size: int = 512,\n",
    "                 num_layers: int = 8, mapping_type: MappingType = MappingType.MLP):\n",
    "        super(ClipCaptionModel, self).__init__()\n",
    "        self.prefix_length = prefix_length\n",
    "        self.gpt = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "        self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n",
    "        self.clip_project = TransformerMapper(prefix_size, self.gpt_embedding_size, prefix_length,\n",
    "                                                                     clip_length, num_layers)\n",
    "\n",
    "\n",
    "class ClipCaptionPrefix(ClipCaptionModel):\n",
    "    def parameters(self, recurse: bool = True):\n",
    "        return self.clip_project.parameters()\n",
    "\n",
    "    def train(self, mode: bool = True):\n",
    "        super(ClipCaptionPrefix, self).train(mode)\n",
    "        self.gpt.eval()\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load the CLIP model and GPT-2 tokenizer\n",
    "clip_model, preprocess = clip.load(\"RN50x4\", device=\"cuda\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Define the Transformer model\n",
    "class VideoTransformer(nn.Module):\n",
    "    def __init__(self, clip_dim, time_dim, hidden_dim, nhead, num_layers):\n",
    "        super().__init__()\n",
    "        self.clip_dim = clip_dim\n",
    "        self.time_dim = time_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.nhead = nhead\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Define the input linear layer\n",
    "        self.input_linear = nn.Linear(clip_dim + time_dim, hidden_dim)\n",
    "\n",
    "        # Define the Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(hidden_dim, nhead)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "\n",
    "        # Define the output linear layer\n",
    "        self.output_linear = nn.Linear(hidden_dim, tokenizer.vocab_size)\n",
    "\n",
    "    def forward(self, clip_features, time_features):\n",
    "        # Concatenate the CLIP features and time features\n",
    "        x = torch.cat((clip_features, time_features), dim=-1)\n",
    "\n",
    "        # Pass the input through the input linear layer\n",
    "        x = self.input_linear(x)\n",
    "\n",
    "        # Pass the input through the Transformer encoder\n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        # Pass the output through the output linear layer\n",
    "        x = self.output_linear(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Instantiate the Transformer model\n",
    "model = VideoTransformer(clip_model.visual.output_dim, 1, 512, 8, 6)\n",
    "\n",
    "# Load a video and extract frames\n",
    "video_frames = load_video_frames(\"my_video.mp4\")\n",
    "\n",
    "# Process each frame using CLIP and the Transformer model\n",
    "for i, frame in enumerate(video_frames):\n",
    "    # Preprocess the frame using CLIP's preprocess function\n",
    "    frame = preprocess(frame)\n",
    "\n",
    "    # Compute the CLIP features for the frame\n",
    "    with torch.no_grad():\n",
    "        clip_features = clip_model.encode_image(frame)\n",
    "\n",
    "    # Compute the time feature for the frame (frame number divided by frame rate)\n",
    "    time_feature = torch.tensor([[i / FRAME_RATE]])\n",
    "\n",
    "    # Pass the CLIP features and time feature through the Transformer model\n",
    "    output_token_logits = model(clip_features.unsqueeze(0), time_feature.unsqueeze(0))\n",
    "\n",
    "    # Compute the most likely output token\n",
    "    output_token = torch.argmax(output_token_logits, dim=-1).item()\n",
    "\n",
    "    # Decode the output token using GPT-2's decode function\n",
    "    output_text = tokenizer.decode(output_token)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clipcap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
