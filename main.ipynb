{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dhavi\\anaconda3\\envs\\clipcap\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from tqdm import tqdm, trange\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #csv_file\n",
    "# #tensor_dir\n",
    "\n",
    "# #Data loaders Pytorch dataset class\n",
    "# class VideoDescriptionDataset(Dataset):\n",
    "#     def __init__(self, csv_file, tensor_dir):\n",
    "#         self.df = pd.read_csv(csv_file)\n",
    "#         self.tensor_dir = tensor_dir\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.df)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         if torch.is_tensor(idx):\n",
    "#             idx = idx.tolist()\n",
    "        \n",
    "#         tensor_file = os.path.join(self.tensor_dir, str(self.df.loc[idx, \"id\"]) + '.pt')\n",
    "#         tensor = torch.load(tensor_file)\n",
    "#         description = self.df.loc[idx, \"description\"]\n",
    "\n",
    "#         return tensor, description\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "\n",
    "# Data loaders Pytorch dataset class\n",
    "class VideoDescriptionDataset(Dataset):\n",
    "    def __init__(self, csv_file, tensor_dir, gpt2_tokenizer):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.tensor_dir = tensor_dir\n",
    "        self.gpt2_tokenizer = gpt2_tokenizer\n",
    "        self.error = []\n",
    "\n",
    "        print(\"test\", len(self.df), self.tensor_dir, self.gpt2_tokenizer)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            # print(\"A\")\n",
    "\n",
    "        print(\"B\")\n",
    "\n",
    "        tensor_file = (self.tensor_dir + str(self.df.loc[idx, \"id\"]) + '.pt')\n",
    "        print(\"id is\" + str(self.df.loc[idx, \"id\"]))\n",
    "        # print(f\"tensor file is :{tensor_file}\")\n",
    "\n",
    "        print(\"C\")\n",
    "\n",
    "        try:\n",
    "            print(\"D\")\n",
    "            tensor = torch.load(tensor_file).to(device)\n",
    "            print(\"E\")\n",
    "            description = self.df.loc[idx, \"descriptions\"]\n",
    "            print(\"F\")\n",
    "            # Tokenize the description using GPT-2 tokenizer\n",
    "            tokens = self.gpt2_tokenizer(description, return_tensors='pt').to(device)\n",
    "            print(\"G\")\n",
    "            \n",
    "            return tensor, tokens[\"input_ids\"]\n",
    "        except Exception as e:\n",
    "            print(\"H\")\n",
    "            print(f\"Error loading data for index {idx}: {str(e)}\")\n",
    "            self.error.append(str(self.df.loc[idx, \"id\"]))\n",
    "            return torch.empty(0),torch.empty(0)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "d_model = 512  # Dimension of the model\n",
    "output_dim = 32  # Output dimension of the MLP\n",
    "num_layers = 6  # Number of transformer layers\n",
    "num_heads = 8  # Number of heads in multi-headed attention\n",
    "dim_feedforward = 2048  # Dimension of the feedforward network\n",
    "dropout = 0.1  # Dropout rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GPT-2 model and tokenizer\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
    "gpt2_model.eval()  # Freeze the GPT-2 model\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        L, N = x.size(1), x.size(0)\n",
    "        pos = torch.arange(L).unsqueeze(0).repeat(N, 1).to(x.device)\n",
    "        pos_embedding = self.calc_pos_embedding(pos)\n",
    "        return x + pos_embedding\n",
    "    \n",
    "    def calc_pos_embedding(self, pos):\n",
    "        pos = pos.float()\n",
    "        factor = torch.exp(-torch.arange(0, self.d_model, 2).float() * (torch.log(torch.tensor(10000.0)) / self.d_model))\n",
    "        sinusoid_inp = torch.ger(pos, factor)\n",
    "        pos_embedding = torch.cat([sinusoid_inp.sin(), sinusoid_inp.cos()], dim=-1)\n",
    "        return pos_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PositionalEncoding(nn.Module):\n",
    "#     def __init__(self, d_model, max_len=5000):\n",
    "#         super(PositionalEncoding, self).__init__()\n",
    "#         pe = torch.zeros(max_len, d_model).to(device)\n",
    "#         position = torch.arange(0, len(d_model)).unsqueeze(1).to(device)\n",
    "#         div_term = torch.exp(torch.arange(0, d_model, 2) * -(torch.log(torch.tensor(10000.0)) / d_model)).to(device)\n",
    "#         pe[:, 0::2] = torch.sin(position * div_term)\n",
    "#         pe[:, 1::2] = torch.cos(position * div_term)\n",
    "#         pe = pe.unsqueeze(0)\n",
    "#         self.register_buffer('pe', pe)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x + self.pe[:, :x.size(1)]\n",
    "#         return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dim_feedforward, dropout, num_layers):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model, num_heads, dim_feedforward, dropout)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers)\n",
    "        self.output_layer = nn.Linear(d_model, output_dim)\n",
    "    \n",
    "    def forward(self, x, tgt):\n",
    "        tgt = tgt.permute(1, 0, 2)\n",
    "        output = self.transformer_decoder(tgt, x)\n",
    "        output = self.output_layer(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc1(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dim_feedforward, dropout, num_layers, output_dim):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        self.decoder = TransformerDecoder(d_model, num_heads, dim_feedforward, dropout, num_layers)\n",
    "        self.mlp = MLP(d_model, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.decoder(x)\n",
    "        x = self.mlp(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 7288 C:/Users/dhavi/Downloads/VIdeoCLIPCap/video_tensors_120/ GPT2Tokenizer(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True)}, clean_up_tokenization_spaces=True)\n"
     ]
    }
   ],
   "source": [
    "# Loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# csv_file = \"C:/Users/dhavi/Downloads/VIdeoCLIPCap/Charades/Charades_gpt_train.csv\"\n",
    "csv_file = \"C:/Users/dhavi/Downloads/VIdeoCLIPCap/Charades/Charades_gpt_train_64.csv\"\n",
    "pt_path = \"C:/Users/dhavi/Downloads/VIdeoCLIPCap/video_tensors_120/\"\n",
    "\n",
    "dataset = VideoDescriptionDataset(csv_file, pt_path, gpt2_tokenizer)\n",
    "\n",
    "# Training loop\n",
    "def Train(dataset: dataset, model: MyModel, lr: float = 2e-5, warmup_steps: int = 5000, output_dir: str = \".\", output_prefix: str = \"\"):\n",
    "    \n",
    "    # Create a SummaryWriter\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    print(\"1\")\n",
    "\n",
    "\n",
    "    # Initialize the DataLoader\n",
    "    dataloader = DataLoader(dataset, batch_size=5, shuffle=True)#, num_workers=2)\n",
    "\n",
    "    print(\"2\")\n",
    "\n",
    "\n",
    "    num_epochs = 10\n",
    "    # Optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    print(\"3\")\n",
    "\n",
    "\n",
    "    # Scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=warmup_steps, num_training_steps=num_epochs * len(dataloader)\n",
    "        )\n",
    "    \n",
    "    print(3.5)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        print(3.75)\n",
    "        # Use tqdm to show progress bar\n",
    "        for i, (input_data, target) in tqdm(enumerate(dataloader)):\n",
    "\n",
    "            if (input_data.shape == torch.Size([])) or (target.shape == torch.Size([])):\n",
    "                print(f\"Skipping batch {i} due to error loading data\")\n",
    "                continue\n",
    "\n",
    "\n",
    "            print(\"3.8\")\n",
    "            input_data, target = input_data.to(device), target.to(device)\n",
    "\n",
    "            print(\"4\")\n",
    "\n",
    "            # Forward pass through your model\n",
    "            output = model(input_data)\n",
    "\n",
    "            print(\"5\")\n",
    "\n",
    "\n",
    "            # Forward pass through GPT-2 model\n",
    "            gpt2_output = gpt2_model(output).last_hidden_state\n",
    "\n",
    "            print(\"6\")  \n",
    "\n",
    "\n",
    "            # Remove the first 32 tokens from the GPT-2 output\n",
    "            gpt2_output = gpt2_output[:, 32:]\n",
    "\n",
    "            print(\"7\")\n",
    "\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = loss_fn(gpt2_output, target)\n",
    "            print(loss)\n",
    "\n",
    "            # Log the loss to TensorBoard\n",
    "            writer.add_scalar('Training Loss', loss.item(), epoch * len(dataloader) + i)\n",
    "\n",
    "            # Backpropagate loss and update parameters\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # Clip gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            # Update learning rate\n",
    "            scheduler.step()\n",
    "\n",
    "            # Save model parameters every 500 iterations\n",
    "            if (i + 1) % 50 == 0:\n",
    "                torch.save(model.state_dict(), os.path.join(output_dir, f'{output_prefix}_epoch_{epoch}_iter_{i + 1}.pt'))\n",
    "\n",
    "        print(f'Epoch: {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "        # Save model checkpoint at the end of each epoch\n",
    "        torch.save(model.state_dict(), os.path.join(output_dir, f'{output_prefix}_epoch_{epoch}.pt'))\n",
    "\n",
    "    # Load the best model checkpoint\n",
    "    model.load_state_dict(torch.load(os.path.join(output_dir, f'{output_prefix}_best.pt')))\n",
    "\n",
    "    # Close the SummaryWriter\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dhavi\\anaconda3\\envs\\clipcap\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "3.5\n",
      "3.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B\n",
      "id isLC1NU\n",
      "C\n",
      "D\n",
      "E\n",
      "F\n",
      "G\n",
      "B\n",
      "id isJ5CAN\n",
      "C\n",
      "D\n",
      "E\n",
      "F\n",
      "G\n",
      "B\n",
      "id isPKEZI\n",
      "C\n",
      "D\n",
      "E\n",
      "F\n",
      "G\n",
      "B\n",
      "id isXYWWG\n",
      "C\n",
      "D\n",
      "E\n",
      "F\n",
      "G\n",
      "B\n",
      "id isPGKB4\n",
      "C\n",
      "D\n",
      "E\n",
      "F\n",
      "G\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [82, 512] at entry 0 and [32, 512] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Create an instance of the model, passing gpt2_model and gpt2_tokenizer as arguments\u001b[39;00m\n\u001b[0;32m      2\u001b[0m model \u001b[39m=\u001b[39m MyModel(d_model, num_heads, dim_feedforward, dropout, num_layers, output_dim)\n\u001b[1;32m----> 4\u001b[0m Train(dataset, model)\n",
      "Cell \u001b[1;32mIn[13], line 43\u001b[0m, in \u001b[0;36mTrain\u001b[1;34m(dataset, model, lr, warmup_steps, output_dir, output_prefix)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m3.75\u001b[39m)\n\u001b[0;32m     42\u001b[0m \u001b[39m# Use tqdm to show progress bar\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m \u001b[39mfor\u001b[39;00m i, (input_data, target) \u001b[39min\u001b[39;00m tqdm(\u001b[39menumerate\u001b[39m(dataloader)):\n\u001b[0;32m     45\u001b[0m     \u001b[39mif\u001b[39;00m (input_data\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m torch\u001b[39m.\u001b[39mSize([])) \u001b[39mor\u001b[39;00m (target\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m torch\u001b[39m.\u001b[39mSize([])):\n\u001b[0;32m     46\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSkipping batch \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m due to error loading data\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\dhavi\\anaconda3\\envs\\clipcap\\Lib\\site-packages\\tqdm\\std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dhavi\\anaconda3\\envs\\clipcap\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\dhavi\\anaconda3\\envs\\clipcap\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_fetcher\u001b[39m.\u001b[39mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\dhavi\\anaconda3\\envs\\clipcap\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcollate_fn(data)\n",
      "File \u001b[1;32mc:\\Users\\dhavi\\anaconda3\\envs\\clipcap\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    205\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39mdefault_collate_fn_map)\n",
      "File \u001b[1;32mc:\\Users\\dhavi\\anaconda3\\envs\\clipcap\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\dhavi\\anaconda3\\envs\\clipcap\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\dhavi\\anaconda3\\envs\\clipcap\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[39mif\u001b[39;00m collate_fn_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     \u001b[39mif\u001b[39;00m elem_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 119\u001b[0m         \u001b[39mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map)\n\u001b[0;32m    121\u001b[0m     \u001b[39mfor\u001b[39;00m collate_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    122\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\dhavi\\anaconda3\\envs\\clipcap\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    160\u001b[0m     storage \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_new_shared(numel, device\u001b[39m=\u001b[39melem\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    161\u001b[0m     out \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mnew(storage)\u001b[39m.\u001b[39mresize_(\u001b[39mlen\u001b[39m(batch), \u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(elem\u001b[39m.\u001b[39msize()))\n\u001b[1;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mstack(batch, \u001b[39m0\u001b[39m, out\u001b[39m=\u001b[39mout)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [82, 512] at entry 0 and [32, 512] at entry 1"
     ]
    }
   ],
   "source": [
    "# Create an instance of the model, passing gpt2_model and gpt2_tokenizer as arguments\n",
    "model = MyModel(d_model, num_heads, dim_feedforward, dropout, num_layers, output_dim)\n",
    "\n",
    "Train(dataset, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset.error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.load(\"C:/Users/dhavi/Downloads/VIdeoCLIPCap/video_tensors_120/01O27.pt\")\n",
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clipcap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
