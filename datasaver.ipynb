{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import skimage.io as io\n",
    "import clip\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "clip_model_type = \"ViT-B_32\"\n",
    "out_path = \"./data/Video_CLIP_ViT-B_32.pkl\"\n",
    "frames_no = 10 \n",
    "csv_path = \"./Charades/Charades_v1_train.csv\"\n",
    "video_path = \"./video\" \n",
    "model, preprocess = clip.load(clip_model_type, device=device, jit=False)\n",
    "\n",
    "# load captions and video names\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "#extract frames\n",
    "def extract_frames(video_file, frames_no):\n",
    "    video, audio, info = torchvision.io.read_video(video_file)\n",
    "\n",
    "    num_frames = info[\"video_fps\"] * info[\"video_duration\"]\n",
    "    frame_rate = info[\"video_fps\"]\n",
    "    interval = int(frame_rate * 60 / frames_no)\n",
    "    frames = []\n",
    "    to_pil = torchvision.transforms.ToPILImage()\n",
    "    for i in range(0, num_frames, interval):\n",
    "      frames.append(to_pil(video[i]))\n",
    "\n",
    "    return frames\n",
    "\n",
    "def encode_frames(frames):\n",
    "\n",
    "    tensors = []\n",
    "\n",
    "    for i in range(1,len(frames)):\n",
    "        frames[i] = preprocess(frames[i]).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#load videos\n",
    "\n",
    "\n",
    "for i in tqdm(range(len(data))):\n",
    "    d = data[i]\n",
    "    img_id = d[\"image_id\"]\n",
    "    filename = f\"./data/coco/train2014/COCO_train2014_{int(img_id):012d}.jpg\"\n",
    "    if not os.path.isfile(filename):\n",
    "        filename = f\"./data/coco/val2014/COCO_val2014_{int(img_id):012d}.jpg\"\n",
    "    image = io.imread(filename)\n",
    "    image = preprocess(Image.fromarray(image)).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        prefix = clip_model.encode_image(image).cpu()\n",
    "    d[\"clip_embedding\"] = i\n",
    "    all_embeddings.append(prefix)\n",
    "    all_captions.append(d)\n",
    "    if (i + 1) % 10000 == 0:\n",
    "        with open(out_path, 'wb') as f:\n",
    "            pickle.dump({\"clip_embedding\": torch.cat(all_embeddings, dim=0), \"captions\": all_captions}, f)\n",
    "\n",
    "with open(out_path, 'wb') as f:\n",
    "    pickle.dump({\"clip_embedding\": torch.cat(all_embeddings, dim=0), \"captions\": all_captions}, f)\n",
    "\n",
    "print('Done')\n",
    "print(\"%0d embeddings saved \" % len(all_embeddings))\n",
    "return 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import clip\n",
    "\n",
    "# Define the parameters\n",
    "x = 10 # Number of frames per minute to extract\n",
    "video_path = \"videos/\" # Path to the folder containing the videos\n",
    "output_path = \"output/\" # Path to the folder where the output tensors will be saved\n",
    "\n",
    "# Load the CLIP model and preprocess\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=\"cuda\")\n",
    "\n",
    "# Define a function to extract frames from a video\n",
    "def extract_frames(video_file, x):\n",
    "  # Load the video using torchvision\n",
    "  video, audio, info = torchvision.io.read_video(video_file)\n",
    "\n",
    "  # Get the number of frames and the frame rate\n",
    "  num_frames = info[\"video_fps\"] * info[\"video_duration\"]\n",
    "  frame_rate = info[\"video_fps\"]\n",
    "\n",
    "  # Calculate the interval between frames to extract\n",
    "  interval = int(frame_rate * 60 / x)\n",
    "\n",
    "  # Initialize an empty list to store the extracted frames\n",
    "  frames = []\n",
    "\n",
    "  # Loop through the video and append every interval-th frame to the list\n",
    "  for i in range(0, num_frames, interval):\n",
    "    frames.append(video[i])\n",
    "\n",
    "  # Return the list of frames\n",
    "  return frames\n",
    "\n",
    "\"\"\"# Define a function to encode frames using CLIP\n",
    "def encode_frames(frames):\n",
    "  # Initialize an empty list to store the encoded tensors\n",
    "  tensors = []\n",
    "\n",
    "  # Loop through the frames and preprocess them\n",
    "  for frame in frames:\n",
    "    # Resize and center crop the frame to 224x224 pixels\n",
    "    frame = torchvision.transforms.Resize(224)(frame)\n",
    "    frame = torchvision.transforms.CenterCrop(224)(frame)\n",
    "\n",
    "    # Convert the frame to a tensor and normalize it\n",
    "    frame = torchvision.transforms.ToTensor()(frame)\n",
    "    frame = torchvision.transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))(frame)\n",
    "\n",
    "    # Append the frame tensor to the list\n",
    "    tensors.append(frame)\n",
    "\n",
    "  # Stack the tensors into a batch\n",
    "  batch = torch.stack(tensors)\n",
    "\n",
    "  # Encode the batch using CLIP and get the image features\n",
    "  with torch.no_grad():\n",
    "    image_features = model.encode_image(batch)\n",
    "\n",
    "  # Return the image features tensor\n",
    "  return image_features\"\"\"\n",
    "\n",
    "def encode_frames(frames):\n",
    "  # Initialize an empty list to store the tensors\n",
    "  tensors = []\n",
    "\n",
    "  # Loop through the frames and preprocess them using CLIP\n",
    "  for frame in frames:\n",
    "    # Convert the frame to a tensor\n",
    "    frame = torchvision.transforms.ToTensor()(frame)\n",
    "\n",
    "    # Preprocess the frame using CLIP\n",
    "    frame = preprocess(frame)\n",
    "\n",
    "    # Append the frame tensor to the list\n",
    "    tensors.append(frame)\n",
    "\n",
    "  # Stack the tensors into a batch\n",
    "  batch = torch.stack(tensors)\n",
    "\n",
    "  # Encode the batch using CLIP and get the image features\n",
    "  with torch.no_grad():\n",
    "    image_features = model.encode_image(batch)\n",
    "\n",
    "  # Return the image features tensor\n",
    "  return image_features\n",
    "\n",
    "# Define a function to concatenate tensors along a dimension\n",
    "def concatenate_tensors(tensors, dim):\n",
    "  # Stack the tensors along the specified dimension\n",
    "  concatenated_tensor = torch.stack(tensors, dim=dim)\n",
    "\n",
    "  # Return the concatenated tensor\n",
    "  return concatenated_tensor\n",
    "\n",
    "# Loop through the videos in the folder\n",
    "for video_file in os.listdir(video_path):\n",
    "  # Extract x frames per minute from the video\n",
    "  frames = extract_frames(video_path + video_file, x)\n",
    "\n",
    "  # Encode the frames using CLIP\n",
    "  image_features = encode_frames(frames)\n",
    "\n",
    "  # Concatenate the image features along dimension 1 to make a 2D tensor\n",
    "  output_tensor = concatenate_tensors(image_features, dim=1)\n",
    "\n",
    "  # Save the output tensor to a file with the same name as the video file\n",
    "  torch.save(output_tensor, output_path + video_file + \".pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clipcap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
